{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col, udf, avg, max\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputDF = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(\"datathon_tadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputDF.createOrReplaceTempView(\"input_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- day: timestamp (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- p_sessionActivity: integer (nullable = true)\n",
      " |-- p_AddToCart: integer (nullable = true)\n",
      " |-- p_trafficChannel: string (nullable = true)\n",
      " |-- p_sessionDuration: integer (nullable = true)\n",
      " |-- p_pageViews: integer (nullable = true)\n",
      " |-- daysToCheckin: string (nullable = true)\n",
      " |-- osType: integer (nullable = true)\n",
      " |-- osTypeName: string (nullable = true)\n",
      " |-- daysFromPreviousVisit: integer (nullable = true)\n",
      " |-- p_TotalPrice: string (nullable = true)\n",
      " |-- isExclusiveMember: integer (nullable = true)\n",
      " |-- loggedIn: integer (nullable = true)\n",
      " |-- p_MapInteraction: integer (nullable = true)\n",
      " |-- BookingPurchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- day: timestamp (nullable = true)\n",
      " |-- p_sessionActivity: integer (nullable = true)\n",
      " |-- p_AddToCart: integer (nullable = true)\n",
      " |-- p_trafficChannel: string (nullable = true)\n",
      " |-- p_sessionDuration: integer (nullable = true)\n",
      " |-- p_pageViews: integer (nullable = true)\n",
      " |-- daysToCheckin: string (nullable = true)\n",
      " |-- osType: integer (nullable = true)\n",
      " |-- daysFromPreviousVisit: integer (nullable = true)\n",
      " |-- p_TotalPrice: string (nullable = true)\n",
      " |-- isExclusiveMember: integer (nullable = true)\n",
      " |-- loggedIn: integer (nullable = true)\n",
      " |-- p_MapInteraction: integer (nullable = true)\n",
      " |-- BookingPurchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selectDF = inputDF.select(\"user_id\", \"gender\", \"day\", \"p_sessionActivity\", \"p_AddToCart\", \"p_trafficChannel\", \"p_sessionDuration\", \"p_pageViews\", \"daysToCheckin\", \"osType\", \"daysFromPreviousVisit\", \"p_TotalPrice\", \"isExclusiveMember\", \"loggedIn\", \"p_MapInteraction\", \"BookingPurchase\").dropna()\n",
    "selectDF.printSchema()\n",
    "# inputDF.filter(inputDF[\"daysToCheckin\"] != \"NA\").count()\n",
    "# inputDF.select(\"user_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_price = float(selectDF.filter(col(\"p_TotalPrice\") != \"NA\").select(max(\"p_TotalPrice\")).take(1)[0][0])\n",
    "avg_checkin_days = float(selectDF.filter(col(\"daysToCheckin\") != \"NA\").select(max(\"daysToCheckin\")).take(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9996.0 99.0\n"
     ]
    }
   ],
   "source": [
    "print(avg_price, avg_checkin_days)\n",
    "\n",
    "# UDF to filter and replace value\n",
    "def filterNA(cell_val, check_val, replace_val):\n",
    "    print(cell_val)\n",
    "    if (cell_val == check_val):\n",
    "        return replace_val\n",
    "    else:\n",
    "        return float(cell_val)\n",
    "\n",
    "filter_na_df = udf(filterNA, FloatType())\n",
    "cleanedDF = selectDF \\\n",
    ".withColumn(\"cleaned_daysToCheckin\", filter_na_df(\"daysToCheckin\", lit(\"NA\"), lit(avg_checkin_days))) \\\n",
    ".withColumn(\"cleaned_totalPrice\", filter_na_df(\"p_TotalPrice\", lit(\"NA\"), lit(avg_price))) \\\n",
    ".drop(\"daysToCheckin\", \"p_TotalPrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputDF.select(col(\"daysToCheckin\")).distinct().show(inputDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|p_trafficChannel|\n",
      "+----------------+\n",
      "|               O|\n",
      "|               O|\n",
      "|               O|\n",
      "|               O|\n",
      "|               O|\n",
      "|               A|\n",
      "|               A|\n",
      "|               O|\n",
      "|               H|\n",
      "|               O|\n",
      "|               O|\n",
      "|               O|\n",
      "|               O|\n",
      "|               O|\n",
      "|               O|\n",
      "|               H|\n",
      "|               A|\n",
      "|               O|\n",
      "|               O|\n",
      "|               A|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "cleanedDF.select(\"p_trafficChannel\").show()\n",
    "indexer = StringIndexer(inputCol=\"p_trafficChannel\", outputCol=\"trafficChannelIndex\")\n",
    "indexedDF = indexer.fit(cleanedDF).transform(cleanedDF).drop(\"p_trafficChannel\")\n",
    "# indexedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- day: timestamp (nullable = true)\n",
      " |-- p_sessionActivity: integer (nullable = true)\n",
      " |-- p_AddToCart: integer (nullable = true)\n",
      " |-- p_sessionDuration: integer (nullable = true)\n",
      " |-- p_pageViews: integer (nullable = true)\n",
      " |-- osType: integer (nullable = true)\n",
      " |-- daysFromPreviousVisit: integer (nullable = true)\n",
      " |-- isExclusiveMember: integer (nullable = true)\n",
      " |-- loggedIn: integer (nullable = true)\n",
      " |-- p_MapInteraction: integer (nullable = true)\n",
      " |-- BookingPurchase: integer (nullable = true)\n",
      " |-- cleaned_daysToCheckin: float (nullable = true)\n",
      " |-- cleaned_totalPrice: float (nullable = true)\n",
      " |-- trafficChannelIndex: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexedDF.count()\n",
    "indexedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def getMonth(date):\n",
    "    return date.month\n",
    "\n",
    "def checkWeekend(date):\n",
    "    if date.weekday() in [5, 6]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def combinedFeature(sess_act, days_previous):\n",
    "    if (sess_act < 1200):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "combinedFeatureUDF = udf(combinedFeature, IntegerType())\n",
    "\n",
    "getMonthUDF = udf(getMonth, IntegerType())\n",
    "checkWeekendUDF = udf(checkWeekend, IntegerType())\n",
    "\n",
    "#create two new columns\n",
    "indexedDF = indexedDF.withColumn(\"month\", getMonthUDF(\"day\")).withColumn(\"weekend\", checkWeekendUDF(\"day\")).withColumn(\"flag1\", combinedFeatureUDF(\"p_sessionActivity\", \"daysFromPreviousVisit\"))\n",
    "# indexedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "encoder1 = OneHotEncoder(inputCol=\"trafficChannelIndex\", outputCol=\"trafficChannelVec\")\n",
    "encodedDF1 = encoder1.transform(indexedDF)\n",
    "encoder2 = OneHotEncoder(inputCol=\"osType\", outputCol=\"osTypeVec\")\n",
    "encodedDF2 = encoder2.transform(encodedDF1)\n",
    "# encodedDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(30,[0,1,3,4,5,7,...|    0|\n",
      "|(30,[1,3,4,5,7,8,...|    0|\n",
      "|(30,[0,1,3,4,7,8,...|    1|\n",
      "|(30,[1,3,4,5,7,8,...|    0|\n",
      "|(30,[1,2,3,4,5,7,...|    0|\n",
      "|(30,[0,1,3,4,5,7,...|    1|\n",
      "|(30,[0,1,3,4,7,8,...|    0|\n",
      "|(30,[1,2,3,4,5,7,...|    1|\n",
      "|(30,[1,3,4,5,7,8,...|    1|\n",
      "|(30,[0,1,3,4,5,7,...|    0|\n",
      "|(30,[1,3,4,5,7,8,...|    0|\n",
      "|(30,[1,2,3,4,5,7,...|    0|\n",
      "|(30,[0,1,3,4,5,7,...|    0|\n",
      "|(30,[1,3,4,5,7,8,...|    0|\n",
      "|(30,[1,3,4,5,7,8,...|    0|\n",
      "|(30,[1,3,4,5,7,8,...|    1|\n",
      "|(30,[0,1,2,3,4,5,...|    1|\n",
      "|(30,[1,2,3,4,5,7,...|    1|\n",
      "|(30,[0,1,3,4,5,7,...|    1|\n",
      "|(30,[0,1,3,4,5,7,...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#assembler with encoders\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"p_sessionActivity\", \"month\", \"weekend\", \"p_sessionDuration\", \"p_pageViews\", \"daysFromPreviousVisit\", \"p_MapInteraction\", \"cleaned_totalPrice\", \"cleaned_daysToCheckin\", \"trafficChannelVec\", \"osTypeVec\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "#assembler for random forest\n",
    "assemblerRF = VectorAssembler(\n",
    "    inputCols=[\"p_sessionActivity\", \"month\", \"weekend\", \"p_sessionDuration\", \"p_pageViews\", \"osType\", \"daysFromPreviousVisit\", \"isExclusiveMember\", \"p_MapInteraction\", \"trafficChannelIndex\", \"cleaned_totalPrice\", \"cleaned_daysToCheckin\", \"flag1\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "featureDF = assembler.transform(encodedDF2).select(\"features\", col(\"BookingPurchase\").alias(\"label\"))\n",
    "# featureDF = assemblerRF.transform(indexedDF).select(\"features\", col(\"BookingPurchase\").alias(\"label\"))\n",
    "\n",
    "# presentDF = featureDF.filter(col(\"label\") == 1).sample(True, 0.3).repartition(3).coalesce(1)\n",
    "# featureDF = featureDF.union(presentDF)\n",
    "featureDF.show()\n",
    "# presentDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                                                |label|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|(30,[0,1,3,4,5,7,8,9,22],[0.031806892473825295,0.18837996266348717,0.007632615014417102,0.0808888606902895,0.4732122249271911,1.028520425828437,2.8310177887019776,2.0297765712573645,2.45956916856625])|0    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# Normalize each Vector using $L^1$ norm.\n",
    "# normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "# normDF = normalizer.transform(featureDF)\n",
    "# print(\"Normalized using L^1 norm\")\n",
    "\n",
    "# normDF = featureDF\n",
    "# scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "#                         withStd=True, withMean=False)\n",
    "\n",
    "normDF = featureDF\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "\n",
    "\n",
    "# normDF = transformedDF\n",
    "# scaler = StandardScaler(inputCol=\"transformedFeatures\", outputCol=\"scaledFeatures\",\n",
    "#                         withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(normDF)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledDF1 = scalerModel.transform(normDF)\n",
    "\n",
    "scaledDF = scaledDF1.select(col(\"scaledFeatures\").alias(\"features\"), \"label\")\n",
    "scaledDF.show(1, truncate=False)\n",
    "# vindexer = VectorIndexer(inputCol=\"scaledFeatures\", outputCol=\"indexedFeatures\", maxCategories=10)\n",
    "# indexerModel = vindexer.fit(scaledDF1)\n",
    "\n",
    "# categoricalFeatures = indexerModel.categoryMaps\n",
    "# print(\"Chose %d categorical features: %s\" %\n",
    "#       (len(categoricalFeatures), \", \".join(str(k) for k in categoricalFeatures.keys())))\n",
    "\n",
    "# # Create new column \"indexed\" with categorical values transformed to indices\n",
    "# indexedData = indexerModel.transform(scaledDF1)\n",
    "# scaledDF = indexedData\n",
    "# scaledDF = normDF\n",
    "# scaledDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import numpy as np\n",
    "# Create some vector data; also works for sparse vectors\n",
    "import math\n",
    "\n",
    "numFeatures = 31\n",
    "arr = [1.0]*numFeatures\n",
    "arr[0] *= math.pow(10.0, 7)\n",
    "arr[1] *= math.pow(10.0, 5)\n",
    "arr[2] *= math.pow(10.0, 5)\n",
    "arr[3] *= math.pow(10.0, 3)\n",
    "arr[4] *= math.pow(10.0, 7)\n",
    "arr[5] *= math.pow(10.0, 6)\n",
    "arr[6] *= math.pow(10.0, 4)\n",
    "arr[8] *= math.pow(10.0, 2)\n",
    "for i in range(9, 14):\n",
    "    arr[i] *= 4.0\n",
    "for i in range(14, len(arr)-1):\n",
    "    arr[i] *= 2.0\n",
    "arr[len(arr)-1] *= math.pow(10.0, 7)\n",
    "\n",
    "\n",
    "\n",
    "transformer = ElementwiseProduct(scalingVec=arr,\n",
    "                                 inputCol=\"scaledFeatures\", outputCol=\"transformedFeatures\")\n",
    "# Batch transform the vectors to create new column:\n",
    "transformedDF = transformer.transform(scaledDF1)\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "mmscaler = MinMaxScaler(inputCol=\"transformedFeatures\", outputCol=\"scaledFeatures2\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = mmscaler.fit(transformedDF)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledDF = scalerModel.transform(transformedDF)\n",
    "# print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "scaledDF.select(col(\"scaledFeatures2\").alias(\"features\"), \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                      |label|scaledFeatures                                                                                                                                                                                                               |transformedFeatures                                                                                                                                                                                                     |scaledFeatures2                                                                                                                                                                                                                                 |\n",
      "+------------------------------------------------------------------------------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(31,[0,1,3,4,5,7,8,9,22],[1.0,1.0,73.0,1.0,10.0,9996.0,99.0,1.0,1.0])         |0    |(31,[0,1,3,4,5,7,8,9,22],[0.031806892473825295,0.18837996266348717,0.007632615014417102,0.0808888606902895,0.4732122249271911,1.028520425828437,2.8310177887019776,2.0297765712573645,2.45956916856625])                     |(31,[0,1,3,4,5,7,8,9,22],[318068.92473825294,18837.996266348717,7.632615014417102,808888.606902895,473212.2249271911,1.028520425828437,283.1017788701978,8.119106285029458,4.9191383371325])                            |[1.4814814814814815E-4,0.0,0.0,8.121398215517433E-4,3.123048094940662E-4,0.09259259259259259,0.0,0.0013238402491934586,0.1371191135734072,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,0.0,1.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,0.0]              |\n",
      "|(31,[1,3,4,5,7,8,9,21,30],[1.0,234.0,4.0,42.0,9996.0,99.0,1.0,1.0,1.0])       |0    |(31,[1,3,4,5,7,8,9,21,30],[0.18837996266348717,0.02446619059415893,0.323555442761158,1.9874913446942026,1.028520425828437,2.8310177887019776,2.0297765712573645,2.001048548518183,2.054401551127408])                        |(31,[1,3,4,5,7,8,9,21,30],[18837.996266348717,24.466190594158927,3235554.42761158,1987491.3446942025,1.028520425828437,283.1017788701978,8.119106285029458,4.002097097036366,2.054401551127408E7])                      |[0.0,0.0,0.0,0.0026032975101795606,0.0012492192379762648,0.38888888888888884,0.0,0.0013238402491934586,0.1371191135734072,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,1.0]                              |\n",
      "|(31,[0,1,3,4,7,8,9,21,30],[53.0,12.0,341.0,5.0,166.0,123.0,1.0,1.0,1.0])      |1    |(31,[0,1,3,4,7,8,9,21,30],[1.6857653011127407,2.2605595519618458,0.035653722190633313,0.4044443034514475,0.017080271177222946,3.517325131417609,2.0297765712573645,2.001048548518183,2.054401551127408])                     |(31,[0,1,3,4,7,8,9,21,30],[1.685765301112741E7,226055.95519618457,35.65372219063331,4044443.034514475,0.017080271177222946,351.7325131417609,8.119106285029458,4.002097097036366,2.054401551127408E7])                  |[0.007851851851851853,1.0,0.0,0.0037936942349197874,0.001561524047470331,0.0,0.0,2.1984541953392772E-5,0.17036011080332408,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,1.0]                             |\n",
      "|(31,[1,3,4,5,7,8,9,21,30],[11.0,778.0,1.0,1.0,9996.0,99.0,1.0,1.0,1.0])       |0    |(31,[1,3,4,5,7,8,9,21,30],[2.0721795892983588,0.08134485590707541,0.0808888606902895,0.04732122249271911,1.028520425828437,2.8310177887019776,2.0297765712573645,2.001048548518183,2.054401551127408])                       |(31,[1,3,4,5,7,8,9,21,30],[207217.95892983588,81.34485590707541,808888.606902895,47321.22249271911,1.028520425828437,283.1017788701978,8.119106285029458,4.002097097036366,2.054401551127408E7])                        |[0.0,0.9090909090909092,0.0,0.008655407961195292,3.123048094940662E-4,0.009259259259259259,0.0,0.0013238402491934586,0.1371191135734072,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,1.0]                |\n",
      "|(31,[1,2,3,4,5,7,8,9,21],[12.0,1.0,34.0,1.0,2.0,9996.0,99.0,1.0,1.0])         |0    |(31,[1,2,3,4,5,7,8,9,21],[2.2605595519618458,2.314473747563794,0.0035549165820572804,0.0808888606902895,0.09464244498543822,1.028520425828437,2.8310177887019776,2.0297765712573645,2.001048548518183])                      |(31,[1,2,3,4,5,7,8,9,21],[226055.95519618457,231447.3747563794,3.5549165820572806,808888.606902895,94642.44498543823,1.028520425828437,283.1017788701978,8.119106285029458,4.002097097036366])                          |[0.0,1.0,1.0,3.7825690318848325E-4,3.123048094940662E-4,0.018518518518518517,0.0,0.0013238402491934586,0.1371191135734072,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,0.0]                              |\n",
      "|(31,[0,1,3,4,5,7,8,10,21],[1.0,1.0,375.0,1.0,2.0,9996.0,99.0,1.0,1.0])        |1    |(31,[0,1,3,4,5,7,8,10,21],[0.031806892473825295,0.18837996266348717,0.03920863877269059,0.0808888606902895,0.09464244498543822,1.028520425828437,2.8310177887019776,2.2195589515089704,2.001048548518183])                   |(31,[0,1,3,4,5,7,8,10,21],[318068.92473825294,18837.996266348717,39.20863877269059,808888.606902895,94642.44498543823,1.028520425828437,283.1017788701978,8.878235806035882,4.002097097036366])                         |[1.4814814814814815E-4,0.0,0.0,0.004171951138108271,3.123048094940662E-4,0.018518518518518517,0.0,0.0013238402491934586,0.1371191135734072,0.0,1.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,0.0]             |\n",
      "|(31,[0,1,3,4,7,8,10,21,30],[39.0,12.0,825.0,20.0,9996.0,99.0,1.0,1.0,1.0])    |0    |(31,[0,1,3,4,7,8,10,21,30],[1.2404688064791864,2.2605595519618458,0.0862590052999193,1.61777721380579,1.028520425828437,2.8310177887019776,2.2195589515089704,2.001048548518183,2.054401551127408])                          |(31,[0,1,3,4,7,8,10,21,30],[1.2404688064791864E7,226055.95519618457,86.2590052999193,1.61777721380579E7,1.028520425828437,283.1017788701978,8.878235806035882,4.002097097036366,2.054401551127408E7])                   |[0.0057777777777777775,1.0,0.0,0.009178292503838194,0.006246096189881324,0.0,0.0,0.0013238402491934586,0.1371191135734072,0.0,1.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,1.0]                              |\n",
      "|(31,[1,2,3,4,5,7,8,9,22,30],[11.0,1.0,320.0,6.0,7.0,9996.0,99.0,1.0,1.0,1.0]) |1    |(31,[1,2,3,4,5,7,8,9,22,30],[2.0721795892983588,2.314473747563794,0.03345803841936264,0.485333164141737,0.3312485574490338,1.028520425828437,2.8310177887019776,2.0297765712573645,2.45956916856625,2.054401551127408])      |(31,[1,2,3,4,5,7,8,9,22,30],[207217.95892983588,231447.3747563794,33.45803841936264,4853331.64141737,331248.55744903383,1.028520425828437,283.1017788701978,8.119106285029458,4.9191383371325,2.054401551127408E7])     |[0.0,0.9090909090909092,1.0,0.0035600649711857242,0.0018738288569643971,0.06481481481481483,0.0,0.0013238402491934586,0.1371191135734072,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,0.0,1.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,1.0]               |\n",
      "|(31,[1,3,4,5,7,8,11,24,30],[11.0,2.0,1.0,14.0,9996.0,99.0,1.0,1.0,1.0])       |1    |(31,[1,3,4,5,7,8,11,24,30],[2.0721795892983588,2.091127401210165E-4,0.0808888606902895,0.6624971148980676,1.028520425828437,2.8310177887019776,3.050433529238338,2.4618173018244858,2.054401551127408])                      |(31,[1,3,4,5,7,8,11,24,30],[207217.95892983588,0.2091127401210165,808888.606902895,662497.1148980677,1.028520425828437,283.1017788701978,12.201734116953352,4.9236346036489715,2.054401551127408E7])                    |[0.0,0.9090909090909092,0.0,2.225040606991078E-5,3.123048094940662E-4,0.12962962962962965,0.0,0.0013238402491934586,0.1371191135734072,0.0,0.0,1.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,0.0,0.0,0.5,1.0,0.0,0.0,0.0,0.0,0.5,1.0]                 |\n",
      "|(31,[0,1,3,4,5,7,8,9,21],[64.0,11.0,35113.0,96.0,3.0,1340.0,135.0,1.0,1.0])   |0    |(31,[0,1,3,4,5,7,8,9,21],[2.035641118324819,2.0721795892983588,3.671287821934626,7.765330626267792,0.14196366747815733,0.13787688781613702,3.860478802775424,2.0297765712573645,2.001048548518183])                          |(31,[0,1,3,4,5,7,8,9,21],[2.035641118324819E7,207217.95892983588,3671.287821934626,7.765330626267792E7,141963.66747815732,0.13787688781613702,386.0478802775424,8.119106285029458,4.002097097036366])                   |[0.009481481481481481,0.9090909090909092,0.0,0.39063925416638856,0.029981261711430354,0.027777777777777776,0.0,1.7746557962377296E-4,0.1869806094182825,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,0.0]|\n",
      "|(31,[1,3,4,5,7,8,9,21],[1.0,242.0,8.0,15.0,9996.0,99.0,1.0,1.0])              |0    |(31,[1,3,4,5,7,8,9,21],[0.18837996266348717,0.025302641554642995,0.647110885522316,0.7098183373907867,1.028520425828437,2.8310177887019776,2.0297765712573645,2.001048548518183])                                            |(31,[1,3,4,5,7,8,9,21],[18837.996266348717,25.302641554642996,6471108.85522316,709818.3373907867,1.028520425828437,283.1017788701978,8.119106285029458,4.002097097036366])                                              |[0.0,0.0,0.0,0.002692299134459204,0.0024984384759525295,0.1388888888888889,0.0,0.0013238402491934586,0.1371191135734072,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,0.0]                                |\n",
      "|(31,[1,2,3,4,5,7,8,9,21],[1.0,1.0,1.0,1.0,13.0,9996.0,99.0,1.0,1.0])          |0    |(31,[1,2,3,4,5,7,8,9,21],[0.18837996266348717,2.314473747563794,1.0455637006050824E-4,0.0808888606902895,0.6151758924053484,1.028520425828437,2.8310177887019776,2.0297765712573645,2.001048548518183])                      |(31,[1,2,3,4,5,7,8,9,21],[18837.996266348717,231447.3747563794,0.10455637006050825,808888.606902895,615175.8924053485,1.028520425828437,283.1017788701978,8.119106285029458,4.002097097036366])                         |[0.0,0.0,1.0,1.112520303495539E-5,3.123048094940662E-4,0.12037037037037038,0.0,0.0013238402491934586,0.1371191135734072,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,0.0]                                |\n",
      "|(31,[0,1,3,4,5,7,8,9,21,30],[14.0,1.0,853.0,15.0,1.0,9996.0,99.0,1.0,1.0,1.0])|0    |(31,[0,1,3,4,5,7,8,9,21,30],[0.44529649463355414,0.18837996266348717,0.08918658366161353,1.2133329103543424,0.04732122249271911,1.028520425828437,2.8310177887019776,2.0297765712573645,2.001048548518183,2.054401551127408])|(31,[0,1,3,4,5,7,8,9,21,30],[4452964.946335541,18837.996266348717,89.18658366161353,1.2133329103543425E7,47321.22249271911,1.028520425828437,283.1017788701978,8.119106285029458,4.002097097036366,2.054401551127408E7])|[0.002074074074074074,0.0,0.0,0.009489798188816945,0.004684572142410992,0.009259259259259259,0.0,0.0013238402491934586,0.1371191135734072,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,1.0]              |\n",
      "|(31,[1,3,4,5,7,8,9,27,30],[11.0,106.0,3.0,1.0,9996.0,99.0,1.0,1.0,1.0])       |0    |(31,[1,3,4,5,7,8,9,27,30],[2.0721795892983588,0.011082975226413874,0.2426665820708685,0.04732122249271911,1.028520425828437,2.8310177887019776,2.0297765712573645,4.482468571728624,2.054401551127408])                      |(31,[1,3,4,5,7,8,9,27,30],[207217.95892983588,11.082975226413874,2426665.820708685,47321.22249271911,1.028520425828437,283.1017788701978,8.119106285029458,8.964937143457249,2.054401551127408E7])                      |[0.0,0.9090909090909092,0.0,0.0011792715217052712,9.369144284821986E-4,0.009259259259259259,0.0,0.0013238402491934586,0.1371191135734072,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,0.0,0.0,0.5,0.0,0.0,0.0,1.0,0.0,0.5,1.0]               |\n",
      "|(31,[1,3,4,5,7,8,9,22],[1.0,7.0,1.0,8.0,9996.0,99.0,1.0,1.0])                 |0    |(31,[1,3,4,5,7,8,9,22],[0.18837996266348717,7.318945904235577E-4,0.0808888606902895,0.3785697799417529,1.028520425828437,2.8310177887019776,2.0297765712573645,2.45956916856625])                                            |(31,[1,3,4,5,7,8,9,22],[18837.996266348717,0.7318945904235576,808888.606902895,378569.7799417529,1.028520425828437,283.1017788701978,8.119106285029458,4.9191383371325])                                                |[0.0,0.0,0.0,7.787642124468772E-5,3.123048094940662E-4,0.07407407407407407,0.0,0.0013238402491934586,0.1371191135734072,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,0.0,1.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,0.0]                                |\n",
      "|(31,[1,3,4,5,7,8,11,21],[11.0,178.0,3.0,6.0,9996.0,24.0,1.0,1.0])             |1    |(31,[1,3,4,5,7,8,11,21],[2.0721795892983588,0.01861103387077047,0.2426665820708685,0.28392733495631467,1.028520425828437,0.686307342715631,3.050433529238338,2.001048548518183])                                             |(31,[1,3,4,5,7,8,11,21],[207217.95892983588,18.61103387077047,2426665.820708685,283927.33495631465,1.028520425828437,68.6307342715631,12.201734116953352,4.002097097036366])                                            |[0.0,0.9090909090909092,0.0,0.0019802861402220596,9.369144284821986E-4,0.05555555555555555,0.0,0.0013238402491934586,0.0332409972299169,0.0,0.0,1.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,0.0]                |\n",
      "|(31,[0,1,2,3,4,5,7,8,10,24],[5.0,1.0,1.0,2375.0,1.0,48.0,9996.0,99.0,1.0,1.0])|1    |(31,[0,1,2,3,4,5,7,8,10,24],[0.15903446236912647,0.18837996266348717,2.314473747563794,0.2483213788937071,0.0808888606902895,2.2714186796505174,1.028520425828437,2.8310177887019776,2.2195589515089704,2.4618173018244858]) |(31,[0,1,2,3,4,5,7,8,10,24],[1590344.6236912648,18837.996266348717,231447.3747563794,248.32137889370708,808888.606902895,2271418.679650517,1.028520425828437,283.1017788701978,8.878235806035882,4.9236346036489715])   |[7.407407407407408E-4,0.0,1.0,0.026422357208019047,3.123048094940662E-4,0.4444444444444444,0.0,0.0013238402491934586,0.1371191135734072,0.0,1.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,0.0,0.0,0.5,1.0,0.0,0.0,0.0,0.0,0.5,0.0]                |\n",
      "|(31,[1,2,3,4,5,7,8,9,22],[1.0,1.0,18.0,1.0,2.0,9996.0,99.0,1.0,1.0])          |1    |(31,[1,2,3,4,5,7,8,9,22],[0.18837996266348717,2.314473747563794,0.0018820146610891485,0.0808888606902895,0.09464244498543822,1.028520425828437,2.8310177887019776,2.0297765712573645,2.45956916856625])                      |(31,[1,2,3,4,5,7,8,9,22],[18837.996266348717,231447.3747563794,1.8820146610891484,808888.606902895,94642.44498543823,1.028520425828437,283.1017788701978,8.119106285029458,4.9191383371325])                            |[0.0,0.0,1.0,2.00253654629197E-4,3.123048094940662E-4,0.018518518518518517,0.0,0.0013238402491934586,0.1371191135734072,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,0.0,1.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,0.0]                                |\n",
      "|(31,[0,1,3,4,5,7,8,9,21],[8.0,1.0,6853.0,7.0,3.0,9996.0,99.0,1.0,1.0])        |1    |(31,[0,1,3,4,5,7,8,9,21],[0.25445513979060236,0.18837996266348717,0.716524804024663,0.5662220248320264,0.14196366747815733,1.028520425828437,2.8310177887019776,2.0297765712573645,2.001048548518183])                       |(31,[0,1,3,4,5,7,8,9,21],[2544551.3979060235,18837.996266348717,716.524804024663,5662220.248320265,141963.66747815732,1.028520425828437,283.1017788701978,8.119106285029458,4.002097097036366])                         |[0.0011851851851851852,0.0,0.0,0.07624101639854929,0.0021861336664584633,0.027777777777777776,0.0,0.0013238402491934586,0.1371191135734072,1.0,0.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,0.0]             |\n",
      "|(31,[0,1,3,4,5,7,8,10,24],[1.0,1.0,134.0,1.0,3.0,9996.0,99.0,1.0,1.0])        |0    |(31,[0,1,3,4,5,7,8,10,24],[0.031806892473825295,0.18837996266348717,0.014010553588108104,0.0808888606902895,0.14196366747815733,1.028520425828437,2.8310177887019776,2.2195589515089704,2.4618173018244858])                 |(31,[0,1,3,4,5,7,8,10,24],[318068.92473825294,18837.996266348717,14.010553588108104,808888.606902895,141963.66747815732,1.028520425828437,283.1017788701978,8.878235806035882,4.9236346036489715])                      |[1.4814814814814815E-4,0.0,0.0,0.001490777206684022,3.123048094940662E-4,0.027777777777777776,0.0,0.0013238402491934586,0.1371191135734072,0.0,1.0,0.0,0.0,0.0,0.5,0.0,0.5,0.5,0.5,0.5,0.5,0.0,0.0,0.5,1.0,0.0,0.0,0.0,0.0,0.5,0.0]             |\n",
      "+------------------------------------------------------------------------------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaledDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\"p_sessionActivity 0\", \"month1\", \"2weekend\", \"3p_sessionDuration\", \"4p_pageViews\", \"5daysFromPreviousVisit\", \"6p_MapInteraction\", \"7cleaned_totalPrice\", \"8cleaned_daysToCheckin\", \"9trafficChannelVec\", \"14osTypeVec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import PCA\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "#         (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "#         (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "# df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "# pca = PCA(k=5, inputCol=\"indexedFeatures\", outputCol=\"pcaFeatures\")\n",
    "# model = pca.fit(scaledDF)\n",
    "\n",
    "# pcaDF = model.transform(scaledDF).select(col(\"pcaFeatures\").alias(\"features\"), \"label\")\n",
    "# pcaDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = featureDF.select(\"features\").rdd.map(lambda row: (row[0].toArray())).collect()\n",
    "y = featureDF.select(\"label\").rdd.map(lambda row: (row[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_X = np.array(X)\n",
    "output_y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_predict = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = svm.SVC(kernel=\"poly\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "train, test = scaledDF.randomSplit([0.95, 0.05], seed=1234)\n",
    "\n",
    "lr = LogisticRegression(maxIter=50)\n",
    "\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.01])\\\n",
    "    .build()\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=BinaryClassificationEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.95)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "tvresult = model.transform(test)\n",
    "tvresult.select(\"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "def mapRow(row):\n",
    "    if (row[0] == int(row[1])): \n",
    "         return 1\n",
    "    else:\n",
    "         return 0\n",
    "        \n",
    "numCorrectPredictions = tvresult.select(\"label\", \"prediction\").rdd.map(lambda row: mapRow(row)).reduce(lambda a, b: a+b)\n",
    "accuracy = 1.0 * numCorrectPredictions / tvresult.count()\n",
    "print(\"Test set accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (31,[],[])\n",
      "Intercept: -1.2935078396500554\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "train, test = scaledDF.randomSplit([0.95, 0.05], seed=1234)\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.5208424620326574\n",
      "+---+---+\n",
      "|FPR|TPR|\n",
      "+---+---+\n",
      "|0.0|0.0|\n",
      "|1.0|1.0|\n",
      "|1.0|1.0|\n",
      "+---+---+\n",
      "\n",
      "areaUnderROC: 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression_419d8dd08d28d66ea194"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1968.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 1130.0 failed 1 times, most recent failure: Lost task 5.0 in stage 1130.0 (TID 4425, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException\n\tat java.lang.System.arraycopy(Native Method)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:628)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:627)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3.apply(Layer.scala:627)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3.apply(Layer.scala:623)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1157)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimize(LBFGS.scala:142)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:817)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:260)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:145)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:72)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat java.lang.System.arraycopy(Native Method)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:628)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:627)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3.apply(Layer.scala:627)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3.apply(Layer.scala:623)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-2ad0d4461a22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# compute accuracy on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1968.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 1130.0 failed 1 times, most recent failure: Lost task 5.0 in stage 1130.0 (TID 4425, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException\n\tat java.lang.System.arraycopy(Native Method)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:628)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:627)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3.apply(Layer.scala:627)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3.apply(Layer.scala:623)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1157)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimize(LBFGS.scala:142)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:817)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:260)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:145)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:72)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat java.lang.System.arraycopy(Native Method)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:628)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:627)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3.apply(Layer.scala:627)\n\tat org.apache.spark.ml.ann.DataStacker$$anonfun$5$$anonfun$apply$3.apply(Layer.scala:623)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#Multi Layer perceptron\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import random\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = scaledDF.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "layers = [31, 25, 20, 10, 5, 2]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234, stepSize=0.03, solver=\"l-bfgs\", initialWeights=[random.random()]*1597)\n",
    "\n",
    "# train the model\n",
    "model = trainer.fit(train)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "percResult = model.transform(test)\n",
    "predictionAndLabels = percResult.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    1|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    1|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 25 rows\n",
      "\n",
      "Test Accuracy = 0.783883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = scaledDF.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=11, maxDepth=10, subsamplingRate=0.8)\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "rfmodel = rf.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "rfpredictions = rfmodel.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "rfpredictions.select(\"prediction\", \"label\", \"features\").show(25)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(rfpredictions)\n",
    "print(\"Test Accuracy = %g\" % accuracy)\n",
    "percResult.filter(col(\"prediction\") == 1.0).count()\n",
    "\n",
    "# print(metrics.precision())\n",
    "# print(metrics.confusionMatrix());\n",
    "# rfModel = rfmodel.stages[2]\n",
    "# print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfpredictions.filter(col(\"prediction\") == 1.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR = 0.3112614097659006\n",
      "Area under ROC = 0.5003751671475785\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "data = scaledDF.rdd.map(lambda row: LabeledPoint(row[1], row[0].toArray()))\n",
    "\n",
    "\n",
    "# Split data into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.8, 0.2], seed=11)\n",
    "\n",
    "# Run training algorithm to build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training)\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/wrapper.py\", line 76, in __del__\n",
      "    SparkContext._active_spark_context._gateway.detach(self._java_obj)\n",
      "AttributeError: 'MinMaxScaler' object has no attribute '_java_obj'\n",
      "Exception ignored in: <bound method JavaModelWrapper.__del__ of <pyspark.mllib.evaluation.MulticlassMetrics object at 0x111d530b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/mllib/common.py\", line 142, in __del__\n",
      "    self._sc._gateway.detach(self._java_model)\n",
      "AttributeError: 'MulticlassMetrics' object has no attribute '_sc'\n",
      "/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/mllib/evaluation.py:237: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7843512310411649\n",
      "0.7843512310411649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/mllib/evaluation.py:249: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "met = MulticlassMetrics(predictionAndLabels)\n",
    "print(met.precision())\n",
    "print(met.recall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7843512310411649\n"
     ]
    }
   ],
   "source": [
    "print(met.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    1|[-39.525479947146...|\n",
      "|       0.0|    0|[-20.118808547138...|\n",
      "|       0.0|    0|[-19.711627161750...|\n",
      "|       0.0|    0|[-16.321780374979...|\n",
      "|       0.0|    1|[-15.695436420146...|\n",
      "|       0.0|    0|[-15.134918020774...|\n",
      "|       0.0|    1|[-15.039741262736...|\n",
      "|       0.0|    1|[-14.883204768102...|\n",
      "|       0.0|    1|[-14.294196881815...|\n",
      "|       0.0|    0|[-14.136479189058...|\n",
      "|       0.0|    0|[-14.052927001294...|\n",
      "|       0.0|    0|[-13.697260991265...|\n",
      "|       0.0|    1|[-13.398598092778...|\n",
      "|       0.0|    1|[-13.148384474778...|\n",
      "|       0.0|    1|[-12.530963640443...|\n",
      "|       0.0|    0|[-12.385137760786...|\n",
      "|       0.0|    0|[-12.287165404486...|\n",
      "|       0.0|    0|[-11.988140032253...|\n",
      "|       0.0|    1|[-11.893476378028...|\n",
      "|       0.0|    0|[-11.849631540517...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Accuracy = 0.785998\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = pcaDF.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(featuresCol=\"features\", maxIter=50, maxDepth=3, labelCol=\"label\", seed=42)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(20)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g\" % accuracy)\n",
    "\n",
    "# gbtModel = model.stages[1]\n",
    "# print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.7859978815958645\n"
     ]
    }
   ],
   "source": [
    "def mapRow(row):\n",
    "    if (row[0] == int(row[1])): \n",
    "         return 1\n",
    "    else:\n",
    "         return 0\n",
    "        \n",
    "numCorrectPredictions = predictions.select(\"label\", \"prediction\").rdd.map(lambda row: mapRow(row)).reduce(lambda a, b: a+b)\n",
    "accuracy = 1.0 * numCorrectPredictions / predictions.count()\n",
    "print(\"Test set accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|       0.0|    0|[0.0,0.0,0.0,0.0,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Accuracy = 0.786108 \n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'MulticlassClassificationEvaluator_490b986d0b596bc4df17 parameter metricName given invalid value precision.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1774.w.\n: java.lang.IllegalArgumentException: MulticlassClassificationEvaluator_490b986d0b596bc4df17 parameter metricName given invalid value precision.\n\tat org.apache.spark.ml.param.Param.validate(params.scala:77)\n\tat org.apache.spark.ml.param.ParamPair.<init>(params.scala:528)\n\tat org.apache.spark.ml.param.Param.$minus$greater(params.scala:87)\n\tat org.apache.spark.ml.param.Param.w(params.scala:83)\n\tat sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-c0aff10c5fd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMulticlassClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetricName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision = %g \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transfer_params_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparamMap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_java_param_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_make_java_param_pair\u001b[0;34m(self, param, value)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mjava_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mjava_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'MulticlassClassificationEvaluator_490b986d0b596bc4df17 parameter metricName given invalid value precision.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = scaledDF.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(20)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g \" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.785998 \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
